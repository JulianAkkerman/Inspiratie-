{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulianAkkerman/Inspiratie-/blob/main/Kopie_van_M4LP_Assignment_2_(2025)_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zydk5m0FpaKT"
      },
      "source": [
        "## Assignment 2\n",
        "\n",
        "This is the complete Assignment 2. You are asked to train and test linear and logistic regression models and access lexical resources."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjCDluLhlvC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "~~Delete this text and write instead of it your:~~\n",
        "* ~~group number (same as the file name, for sanity chack)~~\n",
        "* ~~a list of group members names (NOT student IDs)~~\n",
        "* ~~who contributed to which exercises (you don't need to be very detailed)~~"
      ],
      "metadata": {
        "id": "8rvH77dcj-2Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ledaEclUpvzT"
      },
      "source": [
        "To start the assignment, import prerequisite packages:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext==0.15.2"
      ],
      "metadata": {
        "id": "x5UaNDirNp8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f170d0f-dad4-48e0-a315-b0d3fd6a35c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchtext==0.15.2)\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext==0.15.2) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext==0.15.2) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2) (3.31.4)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->torchtext==0.15.2)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->torchtext==0.15.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->torchtext==0.15.2) (1.3.0)\n",
            "Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRh6vltQpvTn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext as text\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import nltk,sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM9Qv40viw_l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import collections, itertools\n",
        "import more_itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all you need. **Do not import anything else at other points** except lexical resources WordNet and FrameNet where suggested.\n",
        "To do this assignment, you are expected to refer to pyTorch and torchtext documentation."
      ],
      "metadata": {
        "id": "RUH0IPgvw9XA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKO1jxm3pv93"
      },
      "source": [
        "# 2.1 Import GloVe word embedding model\n",
        "\n",
        "GloVe contains _static_ word embeddings. This means that the vector is assigned to word types and does not vary in different contexts or for different word senses. Pretrained GloVe word embedding models exist in different sizes. For the purpose of the exercise, we will use the smallest GloVe vectors with 50 dimensions. First, let's download the vectors. This may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIkj-FlUpX9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9e225e-5e87-4032-ed19-c2b0ce41ac66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:45, 5.22MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:11<00:00, 34181.44it/s]\n"
          ]
        }
      ],
      "source": [
        "glovedim=50\n",
        "#If you experience an issue downloading the vectors, try a different download source by uncommenting the following line:\n",
        "#text.vocab.GloVe.url[\"6B\"] = \"https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip\"\n",
        "vec = text.vocab.GloVe(name='6B', dim=glovedim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(vec.dim)"
      ],
      "metadata": {
        "id": "ZzilXERqaOoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b05a78dc-f8a4-4028-df76-07f3d7444e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPoOZtlxqII4"
      },
      "source": [
        "**Exercise.** How many words do the GloVe embeddings contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JMWM-iTqHNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de3d3f7-27b7-4b35-dd9b-ccd7035f7d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n"
          ]
        }
      ],
      "source": [
        "print(vec.__len__())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. We can now check if particular orthographic words have vectors in `vec`. Check if strings 'asdfdfasd', 'catc', 'cact', and 'cat' have a glove vector, and print the resuts"
      ],
      "metadata": {
        "id": "6iaJetpi504A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_list = ['asdfdfasd', 'catc', 'cact', 'cat']\n",
        "tokens_vector = vec.get_vecs_by_tokens(tokens_list)\n",
        "\n",
        "for vector in tokens_vector:\n",
        "    print(vector)"
      ],
      "metadata": {
        "id": "sWPUwXTayYHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c89bf7-b385-4d5f-87c0-06502a33e5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "tensor([-0.9832, -0.5770, -0.4891, -0.2312, -0.7746, -0.2599,  0.8804, -0.6786,\n",
            "         0.0752, -0.6429,  0.2577,  0.2738,  0.5285,  0.2630, -0.7031,  0.3494,\n",
            "        -0.2407,  0.7581,  0.2753,  0.3231, -0.1638, -0.4644, -0.5918, -0.1965,\n",
            "         0.0926,  0.5420,  0.5445, -0.2751, -0.2848,  0.0364, -0.7616, -0.5728,\n",
            "         0.0859,  0.6841, -0.4170,  0.6888,  0.0200,  0.2174, -0.1152, -0.0930,\n",
            "         0.5504, -0.5972,  0.2634,  0.6699,  0.3886, -0.1229,  1.0426, -0.0213,\n",
            "        -0.4894,  0.1489])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "tensor([ 0.4528, -0.5011, -0.5371, -0.0157,  0.2219,  0.5460, -0.6730, -0.6891,\n",
            "         0.6349, -0.1973,  0.3368,  0.7735,  0.9009,  0.3849,  0.3837,  0.2657,\n",
            "        -0.0806,  0.6109, -1.2894, -0.2231, -0.6158,  0.2170,  0.3561,  0.4450,\n",
            "         0.6089, -1.1633, -1.1579,  0.3612,  0.1047, -0.7832,  1.4352,  0.1863,\n",
            "        -0.2611,  0.8328, -0.2312,  0.3248,  0.1449, -0.4455,  0.3350, -0.9595,\n",
            "        -0.0975,  0.4814, -0.4335,  0.6945,  0.9104, -0.2817,  0.4164, -1.2609,\n",
            "         0.7128,  0.2378])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkVR4dXbqOSY"
      },
      "source": [
        "**Exercise**. What is the vector of _cat_?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-EQf6-dqN_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87c5997-74ed-4d77-b809-f49a96b7a6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.4528, -0.5011, -0.5371, -0.0157,  0.2219,  0.5460, -0.6730, -0.6891,\n",
            "         0.6349, -0.1973,  0.3368,  0.7735,  0.9009,  0.3849,  0.3837,  0.2657,\n",
            "        -0.0806,  0.6109, -1.2894, -0.2231, -0.6158,  0.2170,  0.3561,  0.4450,\n",
            "         0.6089, -1.1633, -1.1579,  0.3612,  0.1047, -0.7832,  1.4352,  0.1863,\n",
            "        -0.2611,  0.8328, -0.2312,  0.3248,  0.1449, -0.4455,  0.3350, -0.9595,\n",
            "        -0.0975,  0.4814, -0.4335,  0.6945,  0.9104, -0.2817,  0.4164, -1.2609,\n",
            "         0.7128,  0.2378])\n"
          ]
        }
      ],
      "source": [
        "cat_vector = vec.get_vecs_by_tokens('cat')\n",
        "print(cat_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byR4o0uZuYU0"
      },
      "source": [
        "# 2.2 Linear regression: Concreteness prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MudXBEHPqZQR"
      },
      "source": [
        "Obtain concreteness ratings from the paper\n",
        "\n",
        "Brysbaert, M., Warriner, A., & Kuperman, V. (2014). Concreteness ratings for 40 thousand generally known English word lemmas. BEHAVIOR RESEARCH METHODS, 46 (3), 904–911. https://doi.org/10.3758/s13428-013-0403-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpty1jvvqZxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86327790-9d6c-4c50-cad6-11ab7a7f8959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-09 12:41:12--  https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1646191 (1.6M) [text/plain]\n",
            "Saving to: ‘Concreteness_ratings_Brysbaert_et_al_BRM.txt’\n",
            "\n",
            "\r          Concreten   0%[                    ]       0  --.-KB/s               \rConcreteness_rating 100%[===================>]   1.57M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-09 12:41:12 (29.4 MB/s) - ‘Concreteness_ratings_Brysbaert_et_al_BRM.txt’ saved [1646191/1646191]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/ArtsEngine/concreteness/master/Concreteness_ratings_Brysbaert_et_al_BRM.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTWFbrn6q1gE"
      },
      "source": [
        "This is a tab-separated file with a header. Structured data like this can be conveniently read via DictReader class from csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJzFEjb1q6XK"
      },
      "outputs": [],
      "source": [
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koGwR3hfrNR_"
      },
      "source": [
        "Using DictReader, create lists ```concreteness_words``` and ```concreteness_scores``` of words in the concreteness ratings file that have a GloVe vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwSaCcHBrhjf"
      },
      "outputs": [],
      "source": [
        "concreteness_words=[]\n",
        "concreteness_scores=[]\n",
        "with open(\"Concreteness_ratings_Brysbaert_et_al_BRM.txt\",'r') as concfile:\n",
        "  read_tsv = csv.DictReader(concfile, delimiter=\"\\t\")\n",
        "  # complete the code below\n",
        "  for row in read_tsv:\n",
        "    word=row[\"Word\"]\n",
        "    if word in vec.stoi:\n",
        "      concreteness_words.append(word)\n",
        "      concreteness_scores.append(float(row[\"Conc.M\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many words ended up in your concreteness dataset?"
      ],
      "metadata": {
        "id": "0MQ7uWgvQxL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(concreteness_words)"
      ],
      "metadata": {
        "id": "AUgfX_wrQszF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb33156-5d05-4363-ac81-8ece0de117bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31618"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDKmvggQrq9X"
      },
      "source": [
        "Create train and test partitions of the concreteness data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCoOZ32trriI"
      },
      "outputs": [],
      "source": [
        "conc_words_train, conc_words_test, conc_scores_train, conc_scores_test = train_test_split(concreteness_words,concreteness_scores,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKo12DqNr57Z"
      },
      "source": [
        "Convert data to torch tensor format to use in a regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IExC7ts2r6WF"
      },
      "outputs": [],
      "source": [
        "vecs_train=torch.stack([vec[w] for w in conc_words_train])\n",
        "vecs_test=torch.stack([vec[w] for w in conc_words_test])\n",
        "scores_train=torch.tensor(conc_scores_train)\n",
        "scores_test=torch.tensor(conc_scores_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise** Explain in a couple of sentences what the second and third lines of code in the above cell does.\n",
        "2. Lijn 2 creert 1 tensor object door alle vectoren die bij de test subset horen in een lijst te zetten en die lijst wordt vervolgens\n",
        "    omgezet in een tensor object door middel van stack.\n",
        "\n",
        "3. Lijn 3 zet de train concreteness scores om in een tensor object.\n",
        "YOUR EXPLANATION HERE"
      ],
      "metadata": {
        "id": "imHe_hm06F7H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA41kZdKsKgR"
      },
      "source": [
        "Now we can define linear regression model in pyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1QduXYcsOj3"
      },
      "outputs": [],
      "source": [
        "class Regression(torch.nn.Module):\n",
        "     def __init__(self, input_dim, output_dim):\n",
        "         super(Regression, self).__init__()\n",
        "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "     def forward(self, x):\n",
        "         outputs = self.linear(x)\n",
        "         return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Explain each line of the class definition above. For example, line 3 can be explained as follows:\n",
        "\n",
        "`super(Regression, self).__init__()` initializes a new Regression model as an instance of the parent class `torch.nn.Module`\n",
        "\n",
        "Now explain all other lines of the class code."
      ],
      "metadata": {
        "id": "C7DMcjTF6-Tv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGGf2CJksYPp"
      },
      "source": [
        "A specific linear regression model can have the input dimensionality of our word embeddings and 1-dimensional input (the concretenss score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfQIuGBqsXoG"
      },
      "outputs": [],
      "source": [
        "model = Regression(glovedim,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQcK2QCqsy4P"
      },
      "source": [
        "To train the model, we need a loss function and an optimiser, including the learning rate parameter. We choose Mean Square Error loss and Stochastic Gradient Descent method with the learning rate of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJHqQcuaMcNL"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIuhn2FeNCph"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjjMEoVvtSc0"
      },
      "source": [
        "We can now train our linear regression model using gradient descent.\n",
        "\n",
        "**Exercise**. Complete the code below. After every 5 training steps, evaluate the model, printing out loss and accuracy for the training and test sets. Calculate the accuracy as the percentage of examples where the predicted score of the model differs from the correct score by less than 1. The training may take a minute or so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDQGZSxWsyPH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c682db921e24cb79f7bcc9a10dc33d6",
            "328c2d5795514d179e07cd276b4ea2f0",
            "fa03c5e961f2466bbe963bedc409bd7c",
            "cdfc29dc4cab4ad2ab7e98e2db12c5f3",
            "2472d1aff02b4ccea283cff704c30d78",
            "2bfd446810d148eabe6f5b0988101806",
            "0b5196c67e8a4a8ba92978338fb60d21",
            "85202623570d48a9b8086c31f4c3cbd7",
            "7f6f67ed7da143a5bfa3101207d2ba3f",
            "792ad576483442b7b245bb9a9c6df054",
            "a1bfa4d3836f457ebc8d3a7e964cde99"
          ]
        },
        "outputId": "273141a0-5030-4f33-eb62-867b54649c20"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Epochs:   0%|          | 0/5000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c682db921e24cb79f7bcc9a10dc33d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.661749541759491\n",
            "accuracy: 0.9320050600885516\n",
            "loss: 0.6053734421730042\n",
            "accuracy: 0.9354838709677419\n",
            "loss: 0.5648090839385986\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.535064697265625\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.5130048990249634\n",
            "accuracy: 0.9402277039848197\n",
            "loss: 0.49652299284935\n",
            "accuracy: 0.9402277039848197\n",
            "loss: 0.4841454327106476\n",
            "accuracy: 0.9414927261227072\n",
            "loss: 0.4748154580593109\n",
            "accuracy: 0.9414927261227072\n",
            "loss: 0.46776294708251953\n",
            "accuracy: 0.9411764705882353\n",
            "loss: 0.4624202251434326\n",
            "accuracy: 0.9408602150537635\n",
            "loss: 0.45836561918258667\n",
            "accuracy: 0.9411764705882353\n",
            "loss: 0.4552839994430542\n",
            "accuracy: 0.9411764705882353\n",
            "loss: 0.45293885469436646\n",
            "accuracy: 0.9408602150537635\n",
            "loss: 0.45115208625793457\n",
            "accuracy: 0.9414927261227072\n",
            "loss: 0.4497891962528229\n",
            "accuracy: 0.9408602150537635\n",
            "loss: 0.448748379945755\n",
            "accuracy: 0.9405439595192916\n",
            "loss: 0.4479525685310364\n",
            "accuracy: 0.9399114484503479\n",
            "loss: 0.44734323024749756\n",
            "accuracy: 0.939595192915876\n",
            "loss: 0.4468758702278137\n",
            "accuracy: 0.9402277039848197\n",
            "loss: 0.4465167224407196\n",
            "accuracy: 0.9402277039848197\n",
            "loss: 0.446240097284317\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4460263252258301\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4458606243133545\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4457314908504486\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44563034176826477\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.44555068016052246\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44548720121383667\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.4454363286495209\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44539493322372437\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.4453608989715576\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44533246755599976\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.4453083276748657\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.4452875554561615\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44526931643486023\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44525301456451416\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44523829221725464\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.445224791765213\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.44521215558052063\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.44520023465156555\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4451890289783478\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44517815113067627\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4451676905155182\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44515758752822876\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44514769315719604\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4451380968093872\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4451286196708679\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44511929154396057\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44511017203330994\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44510114192962646\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44509220123291016\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44508346915245056\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450747072696686\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450661838054657\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450576603412628\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450491964817047\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44504082202911377\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450325667858124\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44502443075180054\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450163245201111\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4450083076953888\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44500038027763367\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4449925124645233\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44498464465141296\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44497692584991455\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4449692964553833\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.44496169686317444\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.44495415687561035\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.44494667649269104\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.4449393153190613\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.4449319541454315\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4449247121810913\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4449175000190735\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44491031765937805\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4449032247066498\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4448961913585663\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44488921761512756\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44488227367401123\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44487541913986206\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44486862421035767\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.4448619484901428\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.4448551833629608\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.44484859704971313\n",
            "accuracy: 0.9367488931056294\n",
            "loss: 0.4448419213294983\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4448353946208954\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44482895731925964\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4448224604129791\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44481611251831055\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44480979442596436\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44480347633361816\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447973072528839\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44479110836982727\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447849690914154\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447788894176483\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447728097438812\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447667598724365\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.444760799407959\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447548985481262\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44474905729293823\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44474321603775024\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44473743438720703\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447316825389862\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44472596049308777\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447202980518341\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44471466541290283\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44470909237861633\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4447035491466522\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446980655193329\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44469258189201355\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446871280670166\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446817934513092\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44467636942863464\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44467106461524963\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446658194065094\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446605145931244\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44465529918670654\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446501135826111\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446450173854828\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446398913860321\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446348249912262\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446297585964203\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44462475180625916\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446197748184204\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.44461485743522644\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446099102497101\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446049928665161\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4446001946926117\n",
            "accuracy: 0.9370651486401012\n",
            "loss: 0.4445953071117401\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445905387401581\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445858299732208\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445810616016388\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445763826370239\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.44457170367240906\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445670247077942\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445624053478241\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445578157901764\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.44455328583717346\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.44454875588417053\n",
            "accuracy: 0.937381404174573\n",
            "loss: 0.4445442259311676\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.44453975558280945\n",
            "accuracy: 0.937697659709045\n",
            "loss: 0.4445353150367737\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445308744907379\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445265531539917\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445221424102783\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445177912712097\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445134699344635\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445091784000397\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.44450485706329346\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4445006549358368\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444964826107025\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444921910762787\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444880485534668\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444839656352997\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444797933101654\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444756805896759\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444715976715088\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444675147533417\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.44446349143981934\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.4444594979286194\n",
            "accuracy: 0.9380139152435167\n",
            "loss: 0.44445550441741943\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44445157051086426\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444476068019867\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444437026977539\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444397985935211\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44443589448928833\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44443202018737793\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444282650947571\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444243609905243\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44442060589790344\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444167912006378\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44441309571266174\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4444093108177185\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44440561532974243\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44440191984176636\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44439825415611267\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.444394588470459\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4443909525871277\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4443873465061188\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4443837106227875\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44438016414642334\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44437652826309204\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44437307119369507\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44436952471733093\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4443660080432892\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4443625807762146\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44435906410217285\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4443556070327759\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443521499633789\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443487823009491\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443453848361969\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443419873714447\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443386197090149\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44433528184890747\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44433194398880005\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.444328635931015\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44432535767555237\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44432204961776733\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443187713623047\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443155527114868\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44431230425834656\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443090558052063\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4443058669567108\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44430267810821533\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44429951906204224\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44429636001586914\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44429323077201843\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442901313304901\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442870020866394\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44428393244743347\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44428086280822754\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442777931690216\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44427475333213806\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442717134952545\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44426867365837097\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442656934261322\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44426265358924866\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442597031593323\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442567527294159\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442537724971771\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44425085186958313\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44424790143966675\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442450702190399\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442421793937683\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442392885684967\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442364275455475\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44423356652259827\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44423070549964905\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442279040813446\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44422510266304016\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442223012447357\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442194998264313\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442167580127716\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44421395659446716\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4442112445831299\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.444208562374115\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44420579075813293\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44420307874679565\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44420042634010315\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44419774413108826\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.444195032119751\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44419240951538086\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44418972730636597\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44418713450431824\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44418445229530334\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.444181889295578\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441792964935303\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44417670369148254\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441741406917572\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441715478897095\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441690444946289\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44416648149490356\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.444163978099823\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44416147470474243\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441589117050171\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441564381122589\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441540241241455\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44415152072906494\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44414904713630676\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44414660334587097\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441441595554352\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441417157649994\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441393315792084\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44413694739341736\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44413453340530396\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44413211941719055\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441297948360443\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441274106502533\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44412505626678467\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441227316856384\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441203474998474\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44411808252334595\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441157877445221\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44411346316337585\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441111385822296\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44410887360572815\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4441066086292267\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44410440325737\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44410207867622375\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44409990310668945\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440976679325104\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440954625606537\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440931975841522\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440910220146179\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440888464450836\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440866708755493\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.444084495306015\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440823495388031\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440802037715912\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4440780282020569\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44407591223716736\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44407376646995544\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440716803073883\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44406965374946594\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44406750798225403\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440653920173645\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44406333565711975\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440612494945526\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44405919313430786\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440571665763855\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44405511021614075\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440530836582184\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44405102729797363\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44404903054237366\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44404709339141846\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440450668334961\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440430700778961\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44404110312461853\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44403916597366333\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44403716921806335\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440351724624634\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44403329491615295\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44403132796287537\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44402942061424255\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44402751326560974\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44402557611465454\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44402366876602173\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440217912197113\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440199136734009\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44401800632476807\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44401612877845764\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.444014310836792\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440123736858368\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44401055574417114\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440087378025055\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44400689005851746\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440051019191742\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44400325417518616\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4440014660358429\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44399964809417725\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.443997859954834\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439961314201355\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44399428367614746\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.443992555141449\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439907968044281\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439890384674072\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44398730993270874\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44398555159568787\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439838230609894\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439821243286133\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439803659915924\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439786970615387\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439769685268402\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439752399921417\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439736008644104\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439719617366791\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439702033996582\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.4439685642719269\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44396695494651794\n",
            "accuracy: 0.9383301707779886\n",
            "loss: 0.44396528601646423\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439636468887329\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439620077610016\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44396036863327026\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439587891101837\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44395712018013\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44395551085472107\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44395390152931213\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439523220062256\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44395074248313904\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439491331577301\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44394761323928833\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443945974111557\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44394442439079285\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439429044723511\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439413249492645\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44393980503082275\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439382553100586\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439367353916168\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439351558685303\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439336955547333\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439321458339691\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44393062591552734\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44392919540405273\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44392773509025574\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439261853694916\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439246952533722\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439232647418976\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439217746257782\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439202845096588\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439188539981842\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443917453289032\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443915992975235\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443914532661438\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44391313195228577\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44391170144081116\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44391030073165894\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44390884041786194\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439074695110321\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439060688018799\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44390466809272766\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4439032971858978\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443901926279068\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44390058517456055\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44389912486076355\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438977539539337\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44389644265174866\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438950717449188\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443893700838089\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438924193382263\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438910484313965\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438897371292114\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.443888396024704\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438870847225189\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438857436180115\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438844919204712\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44388318061828613\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438818693161011\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438805878162384\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44387930631637573\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44387802481651306\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438767433166504\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438754916191101\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44387421011924744\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44387295842170715\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438716471195221\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438704550266266\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438691735267639\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438679814338684\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438667595386505\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44386550784111023\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44386425614356995\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438630938529968\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44386184215545654\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44386062026023865\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438594579696655\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44385823607444763\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44385701417922974\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438558518886566\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438546895980835\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438535273075104\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44385236501693726\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44385114312171936\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44385001063346863\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438488185405731\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438476860523224\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.44384652376174927\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438454210758209\n",
            "accuracy: 0.9386464263124604\n",
            "loss: 0.4438442587852478\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44384315609931946\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44384199380874634\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443840891122818\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383975863456726\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383859634399414\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383755326271057\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383639097213745\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438352882862091\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383421540260315\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438331425189972\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383206963539124\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44383102655410767\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44382989406585693\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443828821182251\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44382771849632263\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44382673501968384\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438256323337555\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44382455945014954\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438234865665436\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438224732875824\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44382143020629883\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44382038712501526\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438193738460541\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438183605670929\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44381728768348694\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44381627440452576\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438152611255646\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438142776489258\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438132345676422\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438122808933258\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44381120800971985\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44381025433540344\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438092112541199\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438082277774811\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44380730390548706\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44380635023117065\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4438053369522095\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44380438327789307\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4438033401966095\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4438024163246155\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4438014626502991\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44380050897598267\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44379958510398865\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44379860162734985\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44379767775535583\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437967538833618\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437958002090454\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437948763370514\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443793922662735\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44379299879074097\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44379210472106934\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437911808490753\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437902867794037\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44378939270973206\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44378840923309326\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443787544965744\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437866508960724\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44378581643104553\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437848627567291\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437839984893799\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44378310441970825\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443782240152359\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44378137588500977\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44378042221069336\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437795579433441\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44377875328063965\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437778890132904\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44377702474594116\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44377613067626953\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44377535581588745\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437744617462158\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44377362728118896\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437728226184845\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44377195835113525\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437711238861084\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44377025961875916\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376951456069946\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437686502933502\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376784563064575\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437670111656189\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376617670059204\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376540184020996\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437645971775055\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443763792514801\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376301765441895\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437622129917145\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376140832901\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44376063346862793\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375982880592346\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437590539455414\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437582790851593\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375747442245483\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375675916671753\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375598430633545\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375520944595337\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437544345855713\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437536597251892\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437529444694519\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437521696090698\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375142455101013\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44375067949295044\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374990463256836\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374915957450867\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374844431877136\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374769926071167\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437469244003296\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437462389469147\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374555349349976\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374480843544006\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374406337738037\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374334812164307\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374263286590576\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374191761016846\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374120235443115\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44374051690101624\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437398314476013\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437391459941864\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437384605407715\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437377452850342\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44373705983161926\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44373637437820435\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437357187271118\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437350034713745\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443734347820282\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437336027622223\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437329173088074\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44373229146003723\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437316358089447\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437309801578522\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44373029470443726\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437296390533447\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437289834022522\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372832775115967\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372767210006714\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437270164489746\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437263309955597\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372567534446716\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437250792980194\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372445344924927\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372379779815674\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437231421470642\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372257590293884\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437219500541687\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372129440307617\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44372066855430603\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437200725078583\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44371944665908813\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437188506126404\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44371819496154785\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437176287174225\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44371703267097473\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443716436624527\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44371578097343445\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437152147293091\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44371455907821655\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437140226364136\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44371339678764343\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44371286034584045\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4437122046947479\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44371163845062256\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437110722064972\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44371047616004944\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437099099159241\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437093436717987\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44370874762535095\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437081813812256\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437076151371002\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44370707869529724\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437065124511719\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437059462070465\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44370537996292114\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437048137187958\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437042772769928\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44370368123054504\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44370320439338684\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437026381492615\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437020719051361\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437015652656555\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44370096921920776\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4437004327774048\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436998963356018\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369935989379883\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369885325431824\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369831681251526\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369781017303467\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436972439289093\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436967372894287\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436962306499481\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369569420814514\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369521737098694\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369471073150635\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443694144487381\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436936378479004\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436931610107422\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436926543712616\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436921179294586\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443691611289978\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369110465049744\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436906576156616\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44369009137153625\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368964433670044\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368913769721985\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368863105773926\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368818402290344\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368770718574524\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368723034858704\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368669390678406\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368624687194824\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368577003479004\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368529319763184\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368481636047363\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368430972099304\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436838924884796\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436834156513214\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436829686164856\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436825215816498\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436820447444916\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436815679073334\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368112087249756\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368067383766174\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44368013739585876\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436797499656677\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436793029308319\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436788558959961\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436784088611603\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436779320240021\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367748498916626\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367706775665283\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443676620721817\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436761438846588\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436757564544678\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436752498149872\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367489218711853\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436744153499603\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436739981174469\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436735212802887\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367313385009766\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367268681526184\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436722993850708\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367191195487976\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44367149472236633\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436710476875305\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436706006526947\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436701834201813\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436698257923126\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436693787574768\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366902112960815\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366857409477234\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436682164669037\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366776943206787\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366735219955444\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436669945716858\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366660714149475\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436661899089813\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436657726764679\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366541504859924\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436650276184082\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436646103858948\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366422295570374\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436638355255127\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366347789764404\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443663090467453\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436626732349396\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436623156070709\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436619281768799\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366151094436646\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436611831188202\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366076588630676\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436604082584381\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44366008043289185\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436596930027008\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365930557250977\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436589479446411\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436585605144501\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365817308425903\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365787506103516\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436574876308441\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365713000297546\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436567425727844\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365647435188293\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436560869216919\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365569949150085\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436553120613098\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365501403808594\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436546266078949\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365429878234863\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365397095680237\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436536431312561\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365325570106506\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436529278755188\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365260004997253\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436522424221039\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365194439888\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365158677101135\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436512291431427\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365087151527405\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44365057349205017\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436502456665039\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364991784095764\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364961981773376\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436492621898651\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364896416664124\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436485767364502\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436482787132263\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364795088768005\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436476528644562\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436472952365875\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364696741104126\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364669919013977\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436464011669159\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364604353904724\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364577531814575\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436454474925995\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436451494693756\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364479184150696\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364452362060547\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436442255973816\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436438977718353\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364359974861145\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436433017253876\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436429738998413\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436427056789398\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364240765571594\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364210963249207\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436418116092682\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436414837837219\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364118576049805\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44364091753959656\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436406195163727\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436402916908264\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436400532722473\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363975524902344\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363945722579956\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436391592025757\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436388909816742\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436385929584503\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436383545398712\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363802671432495\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363775849342346\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443637490272522\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436372220516205\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.443636953830719\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363662600517273\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363638758659363\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436361491680145\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363588094711304\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363558292388916\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436352849006653\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436350166797638\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436347782611847\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436344802379608\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436342418193817\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436340034008026\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363370537757874\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363343715667725\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363319873809814\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363293051719666\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.44363269209861755\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436323642730713\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436321258544922\n",
            "accuracy: 0.9392789373814042\n",
            "loss: 0.4436319172382355\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436315894126892\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436313807964325\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436311423778534\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436308443546295\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436306357383728\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436304271221161\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436301589012146\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436298608779907\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443629652261734\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362932443618774\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436291456222534\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436289072036743\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436286687850952\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436284303665161\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443628191947937\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436279535293579\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436277151107788\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436274766921997\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436272382736206\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436269998550415\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436267018318176\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436265230178833\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436263144016266\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436260759830475\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443625807762146\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436255693435669\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436253309249878\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436251223087311\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362494349479675\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362467527389526\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362443685531616\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362425804138184\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436240494251251\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443623811006546\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362354278564453\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362330436706543\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436231553554535\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436228573322296\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436227083206177\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436224699020386\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362226128578186\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362205266952515\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362184405326843\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436216354370117\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436213970184326\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436212182044983\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436209499835968\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436207711696625\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436205327510834\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362032413482666\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44362011551856995\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443619966506958\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436197280883789\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436195492744446\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436193108558655\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361910223960876\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361892342567444\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436187446117401\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436185359954834\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436182975769043\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361814856529236\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361788034439087\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361773133277893\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436175525188446\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436173141002655\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436171352863312\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361692667007446\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361671805381775\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436165690422058\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436163902282715\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361618161201477\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361594319343567\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361579418182373\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443615585565567\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436153769493103\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443615198135376\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361504912376404\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436148703098297\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443614661693573\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361448287963867\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361424446105957\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361412525177\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436139166355133\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361376762390137\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361352920532227\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436134099960327\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436132311820984\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436130225658417\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361287355422974\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361263513565063\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361254572868347\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361230731010437\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361212849617004\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436119794845581\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443611741065979\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361165165901184\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436114430427551\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436112642288208\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436110854148865\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436109662055969\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436107575893402\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361060857772827\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44361045956611633\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443610280752182\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436100721359253\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360995292663574\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436098039150238\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436095952987671\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360944628715515\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436092674732208\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436091184616089\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360893964767456\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443608820438385\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360867142677307\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360846281051636\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436083137989044\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360819458961487\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360795617103577\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436078369617462\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436076581478119\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360753893852234\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436073899269104\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436072111129761\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436070919036865\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436069130897522\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360673427581787\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436066150665283\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443606436252594\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360628724098206\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436061680316925\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436059892177582\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360584020614624\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436056911945343\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360557198524475\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436054229736328\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436052739620209\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360509514808655\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436049461364746\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360482692718506\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436046779155731\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360455870628357\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360437989234924\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436042308807373\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360411167144775\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436039626598358\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436038136482239\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436036944389343\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436035752296448\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360342621803284\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436033070087433\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436030983924866\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443602979183197\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436028301715851\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436027407646179\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443602591753006\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360241293907166\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436023235321045\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360214471817017\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436020255088806\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360193610191345\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436017572879791\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360166788101196\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436015188694\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436013996601105\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436012804508209\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436011016368866\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360101222991943\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436008930206299\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360074400901794\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443600594997406\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360047578811646\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436003565788269\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44360020756721497\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4436000883579254\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359996914863586\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435998499393463\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359973073005676\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435996115207672\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435994625091553\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435993731021881\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359925389289856\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359907507896423\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359898567199707\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359883666038513\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359874725341797\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435986280441284\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359850883483887\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435983896255493\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359830021858215\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435981512069702\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359806180000305\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435979425907135\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359779357910156\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.443597674369812\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359755516052246\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435974359512329\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359734654426575\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435972273349762\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359710812568665\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435969889163971\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.44359689950942993\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435967803001404\n",
            "accuracy: 0.9389626818469323\n",
            "loss: 0.4435966908931732\n",
            "accuracy: 0.9389626818469323\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "losses_test = []\n",
        "Iterations = []\n",
        "epochs = 5000\n",
        "tolerance = 1.0\n",
        "#after defining parameters, we train the model several times on the same data; each iteration is an epoch:\n",
        "for epoch in trange(epochs, desc='Training Epochs'):\n",
        "    x = vecs_train\n",
        "    scores = scores_train\n",
        "    optimizer.zero_grad()\n",
        "    #here we pass the word vectors from the training set, obtaining regression model's predicted outputs:\n",
        "    outputs = model(x)\n",
        "    loss = criterion(torch.squeeze(outputs), scores)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Now, every 5 epochs we can evaluate how the model performs on the data\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        #we don't compute gradients as the model is only evaluated and not updated\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            #complete the code for evaluating the model here\n",
        "            print(f\"loss: {loss}\")\n",
        "\n",
        "            correct = 0\n",
        "\n",
        "            outputs_test = model(vecs_test)\n",
        "\n",
        "            for i in range (outputs_test.__len__()):\n",
        "                if (outputs_test[i]- scores_test[i] <1):\n",
        "                    correct+=1\n",
        "\n",
        "\n",
        "            print(f\"accuracy: {correct/outputs_test.__len__()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SJ3qAwKfDov"
      },
      "source": [
        "**Exercise**. What are the predicted concreteness scores of the nouns _logarithm_ and _mouse_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z76FnkTpAeO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f2d247-add2-42aa-ab49-196f49620144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7750699520111084\n",
            "4.64422082901001\n"
          ]
        }
      ],
      "source": [
        "def predicted_concreteness(wd):\n",
        "  #complete the code here\n",
        "\n",
        "  vec_wd = vec.get_vecs_by_tokens(wd)\n",
        "  model_train = model(vec_wd)\n",
        "\n",
        "\n",
        "\n",
        "  print(float(model_train))\n",
        "\n",
        "predicted_concreteness('logarithm')\n",
        "predicted_concreteness(\"mouse\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6CTQkPAviBj"
      },
      "source": [
        "# 2.3. Create a dataset of WordNet supersenses for words that have GloVe vectors.\n",
        "\n",
        "First, download the WordNet database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_TbA4Pfv9v5",
        "outputId": "07c29e6f-21ea-421e-b217-2437acae2fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read documentation on object types in WordNet, for example by invoking the types:"
      ],
      "metadata": {
        "id": "gPVFTv6lngfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.reader.wordnet.Lemma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "uLnPl63pnK-M",
        "outputId": "c455bbd0-6ee2-4428-ff32-de708b1cf328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.corpus.reader.wordnet.Lemma"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>nltk.corpus.reader.wordnet.Lemma</b><br/>def __init__(wordnet_corpus_reader, synset, name, lexname_index, lex_id, syntactic_marker)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/nltk/corpus/reader/wordnet.py</a>The lexical entry for a single morphological form of a\n",
              "sense-disambiguated word.\n",
              "\n",
              "Create a Lemma from a &quot;&lt;word&gt;.&lt;pos&gt;.&lt;number&gt;.&lt;lemma&gt;&quot; string where:\n",
              "&lt;word&gt; is the morphological stem identifying the synset\n",
              "&lt;pos&gt; is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
              "&lt;number&gt; is the sense number, counting from 0.\n",
              "&lt;lemma&gt; is the morphological form of interest\n",
              "\n",
              "Note that &lt;word&gt; and &lt;lemma&gt; can be different, e.g. the Synset\n",
              "&#x27;salt.n.03&#x27; has the Lemmas &#x27;salt.n.03.salt&#x27;, &#x27;salt.n.03.saltiness&#x27; and\n",
              "&#x27;salt.n.03.salinity&#x27;.\n",
              "\n",
              "Lemma attributes, accessible via methods with the same name:\n",
              "\n",
              "- name: The canonical name of this lemma.\n",
              "- synset: The synset that this lemma belongs to.\n",
              "- syntactic_marker: For adjectives, the WordNet string identifying the\n",
              "  syntactic position relative modified noun. See:\n",
              "  https://wordnet.princeton.edu/documentation/wninput5wn\n",
              "  For all other parts of speech, this attribute is None.\n",
              "- count: The frequency of this lemma in wordnet.\n",
              "\n",
              "Lemma methods:\n",
              "\n",
              "Lemmas have the following methods for retrieving related Lemmas. They\n",
              "correspond to the names for the pointer symbols defined here:\n",
              "https://wordnet.princeton.edu/documentation/wninput5wn\n",
              "These methods all return lists of Lemmas:\n",
              "\n",
              "- antonyms\n",
              "- hypernyms, instance_hypernyms\n",
              "- hyponyms, instance_hyponyms\n",
              "- member_holonyms, substance_holonyms, part_holonyms\n",
              "- member_meronyms, substance_meronyms, part_meronyms\n",
              "- topic_domains, region_domains, usage_domains\n",
              "- attributes\n",
              "- derivationally_related_forms\n",
              "- entailments\n",
              "- causes\n",
              "- also_sees\n",
              "- verb_groups\n",
              "- similar_tos\n",
              "- pertainyms</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 219);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.reader.wordnet.Synset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "e5PBoC8invpj",
        "outputId": "8f161a13-1785-4e0d-d3a2-45505def2117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.corpus.reader.wordnet.Synset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>nltk.corpus.reader.wordnet.Synset</b><br/>def __init__(wordnet_corpus_reader)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/nltk/corpus/reader/wordnet.py</a>Create a Synset from a &quot;&lt;lemma&gt;.&lt;pos&gt;.&lt;number&gt;&quot; string where:\n",
              "&lt;lemma&gt; is the word&#x27;s morphological stem\n",
              "&lt;pos&gt; is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
              "&lt;number&gt; is the sense number, counting from 0.\n",
              "\n",
              "Synset attributes, accessible via methods with the same name:\n",
              "\n",
              "- name: The canonical name of this synset, formed using the first lemma\n",
              "  of this synset. Note that this may be different from the name\n",
              "  passed to the constructor if that string used a different lemma to\n",
              "  identify the synset.\n",
              "- pos: The synset&#x27;s part of speech, matching one of the module level\n",
              "  attributes ADJ, ADJ_SAT, ADV, NOUN or VERB.\n",
              "- lemmas: A list of the Lemma objects for this synset.\n",
              "- definition: The definition for this synset.\n",
              "- examples: A list of example strings for this synset.\n",
              "- offset: The offset in the WordNet dict file of this synset.\n",
              "- lexname: The name of the lexicographer file containing this synset.\n",
              "\n",
              "Synset methods:\n",
              "\n",
              "Synsets have the following methods for retrieving related Synsets.\n",
              "They correspond to the names for the pointer symbols defined here:\n",
              "https://wordnet.princeton.edu/documentation/wninput5wn\n",
              "These methods all return lists of Synsets.\n",
              "\n",
              "- hypernyms, instance_hypernyms\n",
              "- hyponyms, instance_hyponyms\n",
              "- member_holonyms, substance_holonyms, part_holonyms\n",
              "- member_meronyms, substance_meronyms, part_meronyms\n",
              "- attributes\n",
              "- entailments\n",
              "- causes\n",
              "- also_sees\n",
              "- verb_groups\n",
              "- similar_tos\n",
              "\n",
              "Additionally, Synsets support the following methods specific to the\n",
              "hypernym relation:\n",
              "\n",
              "- root_hypernyms\n",
              "- common_hypernyms\n",
              "- lowest_common_hypernyms\n",
              "\n",
              "Note that Synsets do not support the following relations because\n",
              "these are defined by WordNet as lexical relations:\n",
              "\n",
              "- antonyms\n",
              "- derivationally_related_forms\n",
              "- pertainyms</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 351);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Now, retrieve the first lemma corresponding to the adjective _dry_ as `d`:"
      ],
      "metadata": {
        "id": "_nCIbS2Zn7WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synset = wn.synsets(\"dry\")\n",
        "\n",
        "d= synset[0].lemmas()[0]\n",
        "print(d)\n"
      ],
      "metadata": {
        "id": "NRg_XHB6pZf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "9dc74944-ae7d-49c2-850f-db047ebdddc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-23-26619455e7fa>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-26619455e7fa>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    d=\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What lemmas belong to the same SynSet as `d`? Retrieve the list."
      ],
      "metadata": {
        "id": "2pxGTK83pagp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here\n",
        "\n",
        "dry_lemmas = [lemma  for lemma in d.synset().lemmas()]\n",
        "\n",
        "print(dry_lemmas)\n"
      ],
      "metadata": {
        "id": "KN6jSh59phxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Define function `ant_freq` that returns the frequency of (the first) antonym of a lemma. The frequency value should be retrieved from  WordNet ."
      ],
      "metadata": {
        "id": "TjRKZe5yp6WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ant_freq(x):\n",
        "  #your code here"
      ],
      "metadata": {
        "id": "Ijovshizp7Pp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "492eee41-99d3-4bd4-9f46-85d5103cd17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-36-614f912c3ab6>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-614f912c3ab6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    #your code here\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply `ant_freq` to `d`. This will output the frequency of _wet_."
      ],
      "metadata": {
        "id": "W63hTqccovBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ant_freq(d)"
      ],
      "metadata": {
        "id": "xbWZj8cqm3wR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "39ed388f-b91b-4f4b-9740-73cbb7df9449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ant_freq' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-fe8fea7e8010>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mant_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ant_freq' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9UHFcCUwO_4"
      },
      "source": [
        "**Exercise**. Now, create a dataset that includes for each word in WordNet that has a GloVe vector the lexicographic file (supersense) of its first synset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C43EVE68ytWG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "40fbbbf6-d95d-4149-d4f3-9d4ef5efeb07"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-27-aacc66e8c283>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-aacc66e8c283>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    wn_words =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#your code here\n",
        "wn_words =\n",
        "wn_supersenses ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHXhjXvjyt18"
      },
      "source": [
        "Split the dataset into train and test partitions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lrFM_48yW83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ea632d29-17f4-4cef-f474-01ba9ba41b44"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wn_words' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c60c5d3b0f5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn_words_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn_supersenses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn_supersenses_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwn_supersenses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'wn_words' is not defined"
          ]
        }
      ],
      "source": [
        "wn_words_train, wn_words_test, wn_supersenses_train, wn_supersenses_test = train_test_split(wn_words,wn_supersenses,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc0tWUqkugfe"
      },
      "source": [
        "# 2.4. Logistic regression: word class prediction.\n",
        "\n",
        "Now we can address a classification task. Define a (multinomial) regression model using softmax, choose a loss function and an optimizer for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAF9my4vfpYY"
      },
      "source": [
        "**Exercise**. Initialize your model. Use the same Regression class for logistic regression as for linear regression - the difference will come from the objective (loss) function. The output now is not a single number but scores for each of the classes in the classification task. Estimate `num_classes`prior to initializing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7HvScVvazT"
      },
      "outputs": [],
      "source": [
        "num_classes =\n",
        "logreg_model = Regression(glovedim,num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yxodwdYfzTl"
      },
      "source": [
        "Choose the loss function. This is a crucial choice: some of the loss functions in PyTorch (see https://pytorch.org/docs/stable/nn.html) already include a softmax or a sigmoid in their implementaion, which gives computational advantages. Be sure to read the documentation on your loss function to confirm you made a good choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVRdqxAvvIPC"
      },
      "outputs": [],
      "source": [
        "#your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H-JrZZVfzHF"
      },
      "source": [
        "And initialize the optimiser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw6Pt_BKvTFx"
      },
      "outputs": [],
      "source": [
        "#your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSNhlheCk4xN"
      },
      "source": [
        "WordNet is quite big. For efficiency, use the following function for splitting your data into batches when processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbA_NKV1k7vX"
      },
      "outputs": [],
      "source": [
        "batch_size = 200\n",
        "def get_batches(src_iter, tgt_iter, batch_size=batch_size):\n",
        "    for batch in more_itertools.chunked(zip(src_iter, tgt_iter), batch_size):\n",
        "        x, y = zip(*batch)\n",
        "        x = torch.stack(x)\n",
        "        y = torch.stack(y)\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2bMjQiKy04C"
      },
      "source": [
        "**Exercise**. Train and test your logistic regression model, printing the train and test loss and accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpbabReyy3Gg"
      },
      "outputs": [],
      "source": [
        "#your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDWmbmFEh8em"
      },
      "source": [
        "Define a mapping from indices to lexicographic file names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQFoIsxHgExM"
      },
      "outputs": [],
      "source": [
        "#your code\n",
        "itolexname="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the name of lexicographic file 2?"
      ],
      "metadata": {
        "id": "luIUK79GYVzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zriTsMNl-8PC"
      },
      "outputs": [],
      "source": [
        "itolexname[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eJ39Ocgik1Z"
      },
      "source": [
        "**Exercise**. Which supersense (lexicographic file) does your classifier assign to the noun _house_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1JuvmYvgMyk"
      },
      "outputs": [],
      "source": [
        "def predicted_lexname(wd):\n",
        "#your code\n",
        "#\n",
        "\n",
        "predicted_lexname(\"house\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRijRCGLiTyb"
      },
      "source": [
        "Which supersense (lexicographic file) does your classifier assign to the noun _hog_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgePacYfgezw"
      },
      "outputs": [],
      "source": [
        "predicted_lexname(\"hog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL8uxCpyzah1"
      },
      "source": [
        "# 2.5. Hypernymy classification\n",
        "\n",
        "Now, download a lexical entailment (hypernymy) dataset called WBLESS. The dataset was developed by Weeds et al. with the goal of testing models on distinguishing hypernyms from other related word pairs.\n",
        "\n",
        "Weeds et al. (2014) Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. 2014. Learning to distinguish hypernyms and co-hyponyms. In Proceedings of the 2014 International Conference on Computational Linguistics, pages 2249–2259, Dublin, Ireland.\n",
        "\n",
        "WBLESS (together with other relevant datasets) can be conveniently downloaded from the Facebook Research github page by Stephen Roller who worked on hypernymy learning:\n",
        "\n",
        "Stephen Roller, Douwe Kiela, and Maximilian Nickel. 2018. Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora. ACL.\n",
        "\n",
        "Download the tab-separated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UC37FAiq0TN3"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/facebookresearch/hypernymysuite/raw/main/data/wbless.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqCX7YDt27Vs"
      },
      "source": [
        "Check how the data looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfLFN1110Zuc"
      },
      "outputs": [],
      "source": [
        "!head wbless.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8H2DTxmi45e"
      },
      "source": [
        "We used ```csv``` above to process a tab separated file. It can also be done using ```pandas```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcKADH0wT-dJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "76613e8f-8355-4d92-c47a-6136ee661d90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     word1    word2  label relation fold\n",
              "0  frigate    craft   True    hyper  val\n",
              "1  trouble     carp  False    other  val\n",
              "2      fox    mouth  False    other  val\n",
              "3     foot    robin  False    other  val\n",
              "4     vest  garment   True    hyper  val"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71b2d0f6-17ff-49f1-8b00-4d9459f070fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>label</th>\n",
              "      <th>relation</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>frigate</td>\n",
              "      <td>craft</td>\n",
              "      <td>True</td>\n",
              "      <td>hyper</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trouble</td>\n",
              "      <td>carp</td>\n",
              "      <td>False</td>\n",
              "      <td>other</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fox</td>\n",
              "      <td>mouth</td>\n",
              "      <td>False</td>\n",
              "      <td>other</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>foot</td>\n",
              "      <td>robin</td>\n",
              "      <td>False</td>\n",
              "      <td>other</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vest</td>\n",
              "      <td>garment</td>\n",
              "      <td>True</td>\n",
              "      <td>hyper</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71b2d0f6-17ff-49f1-8b00-4d9459f070fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71b2d0f6-17ff-49f1-8b00-4d9459f070fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71b2d0f6-17ff-49f1-8b00-4d9459f070fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5f428f6c-ac48-4d3e-8113-44d8ac422440\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f428f6c-ac48-4d3e-8113-44d8ac422440')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5f428f6c-ac48-4d3e-8113-44d8ac422440 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "wbless_df",
              "summary": "{\n  \"name\": \"wbless_df\",\n  \"rows\": 1668,\n  \"fields\": [\n    {\n      \"column\": \"word1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 441,\n        \"samples\": [\n          \"cranberry\",\n          \"gill\",\n          \"vertebrate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 498,\n        \"samples\": [\n          \"club\",\n          \"ambulance\",\n          \"runner-up\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"other\",\n          \"hyper\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fold\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"test\",\n          \"val\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "wbless_df = pd.read_csv('wbless.tsv', sep='\\t')\n",
        "wbless_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78WU2dWP3GvT"
      },
      "source": [
        "Now create training and test data for relation classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kP1WKoj3HN2"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkUxGPQN2ODk"
      },
      "source": [
        "Split into training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5TADig-3VZ_"
      },
      "outputs": [],
      "source": [
        "wbless_words_train, wbless_words_test, hypernymy_train, hypernymy_test = train_test_split(wbless_words,hypernymy,test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZNxIFOc3kN9"
      },
      "source": [
        "**Exercise**. Create, train and test a logistic regression model that predicts whether two words stand in the hypernymy relation given their GloVe vectors.\n",
        "\n",
        "Make sure your model predicts a single score used for the binary decision (hypernymy vs. non-hypernymy) rather than scores for multiple classes, and choose the loss function in pyTorch accordingly.\n",
        "\n",
        "Print the train and test loss and accuracy. Use the concatenation of the two words' vectors as input to the logistic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkYFJcWH320c"
      },
      "outputs": [],
      "source": [
        "#your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYuoCyuWm-NF"
      },
      "source": [
        "What label does your model predict for the pair _dog,animal_? Your code below should produce a Boolean value, `True` for the positive class (hypernymy) and `False` for the negative class (not hypernymy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Kujrk1QjIJs"
      },
      "outputs": [],
      "source": [
        "def predicted_hypernymy(w1,w2):\n",
        "  #complete the code\n",
        "\n",
        "predicted_hypernymy(\"dog\",\"animal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gR6R_6LnL05"
      },
      "source": [
        "What label does your model predict for the pair _dog,cat_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n40gWmRjuf6"
      },
      "outputs": [],
      "source": [
        "predicted_hypernymy(\"dog\",\"cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCKBTiCCnRRu"
      },
      "source": [
        "What label does your model predict for the pair _animal,dog_?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_9JK2EnnT2d"
      },
      "outputs": [],
      "source": [
        "predicted_hypernymy(\"animal\",\"dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.6 Using FrameNet\n",
        "\n"
      ],
      "metadata": {
        "id": "-xR2nXnH5HJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can explore FrameNet online:\n",
        "https://framenet.icsi.berkeley.edu/frameIndex\n",
        "\n",
        "Or read detailed documentation here:\n",
        "https://framenet2.icsi.berkeley.edu/docs/r1.7/book.pdf\n",
        "\n",
        "Now, load FrameNet via the NLTK package:"
      ],
      "metadata": {
        "id": "nENYVvXQiwMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('framenet_v17')\n",
        "from nltk.corpus import framenet as fn"
      ],
      "metadata": {
        "id": "K1kaoStljcY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fn.lus` allows to retrieve lexical units (LUs) recorded in FrameNet. A lexical unit approximately corresponds to a lemma in WordNet, i.e. a word taken in a specific sense. Without additional parameters, it returns a complete list:\n",
        "\n"
      ],
      "metadata": {
        "id": "K7PE47iCaR2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijewRC_JaShn",
        "outputId": "fc97de4d-772e-4a3d-d7bc-364658083c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<lu ID=16601 name=(can't) help.v>, <lu ID=14632 name=(in/out of) line.n>, ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also retrieve lexical units by regular expression search. For example, the following returns the list of LUs that contain string _pres_ at the beginning of the LU name:"
      ],
      "metadata": {
        "id": "SC2XBdpyauNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lus('^pres')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJCGq6a181-y",
        "outputId": "bd226881-6ed9-45d0-8897-2850eb7a540e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<lu ID=957 name=press.v>, <lu ID=10117 name=press.v>, ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual lexical units can be retrieved by ID, for example:"
      ],
      "metadata": {
        "id": "aKfBkMyPbbLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lu(10117)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUpopayw9AFe",
        "outputId": "2b74acb7-d6e2-4132-c341-fb3b9330bda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lexical unit (10117): press.v\n",
              "\n",
              "[definition]\n",
              "  COD: make strong efforts to persuade or force to do something.\n",
              "\n",
              "[frame] Attempt_suasion(87)\n",
              "\n",
              "[POS] V\n",
              "\n",
              "[status] Finished_Initial\n",
              "\n",
              "[lexemes] press/V\n",
              "\n",
              "[semTypes] 0 semantic types\n",
              "\n",
              "[URL] https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu10117.xml\n",
              "\n",
              "[subCorpus] 12 subcorpora\n",
              "  01-T-Won-(1), 02-T-Wto-(1), 03-T-Winto-(1), 04-T-Wwith-(1),\n",
              "  05-T-Wfor-(1), 06-AVP-T-(1), 07-T-AVP-(1), 08-T-NP-PP-(1),\n",
              "  09-T-NP-(1), manually-added, other-matched-(1), other-\n",
              "  unmatched-(1)\n",
              "\n",
              "[exemplars] 20 sentences across all subcorpora"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terms that show up in square brackets are attributes, in this case attributes of the lexical unit. They include usage examples:"
      ],
      "metadata": {
        "id": "9QHkZL7Xb2me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lu(10117).exemplars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzDT4Cuob5Ku",
        "outputId": "70490aec-6000-4c17-e330-a9004c306b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "exemplar sentences for press.v in Attempt_suasion:\n",
              "\n",
              "[0] The sponsors of the bill made clear their intention to press for a vote on it within the current legislative session .\n",
              "[1] Anne McIntosh , MEP for North East Essex , is pressing the Government to reverse its policy of controlled retreat , which allows the sea to gradually take its natural course .\n",
              "[2] Lewis pressed Adam to accompany him on those solicitous weekend visits but Adam nearly always said he was too busy or would be bored .\n",
              "[3] The committee also agreed to press for changes to the rule which limits the level of right-to-buy discounts in the case of newly built houses or those recently modernised .\n",
              "[4] It is no secret that Damascus has been using the PKK as a bargaining chip to press Turkey into complying with its demands for more water from the Euphrates , on which it heavily depends .\n",
              "[5] The tenant 's adviser should therefore press for an obligation on the landlord to notify the tenant of any application made by him to the President .\n",
              "[6] They press for much needed democratic reforms to enable the citizens to participate .\n",
              "[7] When I press him for more names he suddenly gets the deer-caught-in-the-headlights look , and , deciding he 's already revealed too much , replies , ` Ah , just people . \"\n",
              "[8] In these circumstances the Soviet Union has pressed more recently for a more limited regime for the Gulf involving restrictions on the naval presence of the Great Powers in the region .\n",
              "[9] Mr Gorbachev may refrain from saying it publicly on Saturday but he can be expected to press Mr Honecker all the more urgently in private .\n",
              "[10] The houses of Hohenstaufen ( Ghibellines ) and Welf ( Guelphs ) both pressed their claims to the royal crown but as already described , Frederick Barbarossa succeeded in his claim .\n",
              "[11] BRITISH Petroleum is to press Chancellor Norman Lamont for a reduction in North Sea taxes and is making additional cuts in capital spending to avoid the risks of further cash and dividend pressures .\n",
              "[12] We will press them to put into practice the principles of the Maastricht declaration .\n",
              "[13] Once in office Healey pressed his Nato colleagues to accept that no conflict was possible in Europe at any level between that of a local skirmish and all-out war .\n",
              "[14] Suppose she was pressing him to pay it back because she needed it for her vineyard . \"\n",
              "[15] I would n't press you for a decision , I promise you .\n",
              "[16] We will press for similar standards throughout the European Community and strengthen the work of consumer groups and advice centres so that the aspirations and standards are met .\n",
              "[17] Senior Tories pressed Michael Heseltine , the environment secretary , to discuss radical changes to the poll tax .\n",
              "[18] With membership of the Church of England steadily dwindling , strong-willed vicars are pressing equally strong-willed and often non-religious ringers to attend services .\n",
              "[19] QN : Did Sharon urge the US leader to keep pressing Iran to give up its nuclear program altogether ?\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and the frame that the lexical unit evokes, which in turn has its own attributes:"
      ],
      "metadata": {
        "id": "ef8vlLl_ckrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fn.lu(10117).frame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWKLSgeEbqi6",
        "outputId": "c4d72ac8-8231-4c84-f042-67199f30751b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "frame (87): Attempt_suasion\n",
              "\n",
              "[URL] https://framenet2.icsi.berkeley.edu/fnReports/data/frame/Attempt_suasion.xml\n",
              "\n",
              "[definition]\n",
              "  The  Speaker expresses through language his wish to get the\n",
              "  Addressee to act in some way that will help to bring about events\n",
              "  or states described in the Content. There is no implication that\n",
              "  the Addressee forms an intention to act, let alone acts.     'Mr\n",
              "  Smithers always encourages the employees to stay late and work\n",
              "  harder.'  'Dennis Rodman advises moderation in all things. INI'\n",
              "  The Content most prototypically refers to an action that the\n",
              "  Addressee will carry out themselves, but may (in the case of\n",
              "  valences with a non-finite Content clause) merely refer to a\n",
              "  situation that they have indirect influence over, as in the\n",
              "  following  'When I talked to her, I suggested that he be removed\n",
              "  from office . DNI '\n",
              "\n",
              "[semTypes] 0 semantic types\n",
              "\n",
              "[frameRelations] 8 frame relations\n",
              "  <Parent=Attempt -- Inheritance -> Child=Attempt_suasion>\n",
              "  <Parent=Attempt_suasion -- Using -> Child=Suasion>\n",
              "  <Parent=Communication -- Using -> Child=Attempt_suasion>\n",
              "  <Parent=Subjective_influence -- Using -> Child=Attempt_suasion>\n",
              "  <Source=Attempt_suasion -- ReFraming_Mapping -> Target=Causation>\n",
              "  <Source=Attempt_suasion -- ReFraming_Mapping -> Target=Manipulate_into_doing>\n",
              "  <Source=Attempt_suasion -- ReFraming_Mapping -> Target=Subjective_influence>\n",
              "  <Source=Request -- ReFraming_Mapping -> Target=Attempt_suasion>\n",
              "\n",
              "[lexUnit] 16 lexical units\n",
              "  admonish.v (1809), advise.v (1810), advocate.v (13475), beg.v\n",
              "  (1812), cajole.v (1813), enjoin.v (18519), exhort.v (1818),\n",
              "  lobby.v (13116), press.v (10117), pressure.n (11455), pressure.v\n",
              "  (1820), prevail.v (1821), recommend.v (16934), suggest.v (16340),\n",
              "  suggestion.n (18450), urge.v (1805)\n",
              "\n",
              "\n",
              "[FE] 20 frame elements\n",
              "            Core: Addressee (409), Content (408), Medium (427), Salient_entity (14440), Speaker (410), Topic (414)\n",
              "      Peripheral: Degree (1225), Manner (1222), Means (1224), Place (12626), Purpose (13346), Time (10234)\n",
              "  Extra-Thematic: Circumstances (10237), Depictive (1223), Explanation (10235), Frequency (10511), Group (10508), Period_of_iterations (10236), Re-encoding (10513), Role (15518)\n",
              "\n",
              "[FEcoreSets] 2 frame element core sets\n",
              "  Content, Topic\n",
              "  Speaker, Medium"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Find all LUs on FrameNet that share the frame with the noun *car*. Print these words along with their definitions."
      ],
      "metadata": {
        "id": "Cctg8so28msu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "JiVFBhPqjgFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**. Define a function that takes a FrameNet frame as input and prints out the definitions of all core frame elements associated with the frame. For example for the frame associated with the noun _car_ your function will print the definition of the only core frame element _Vehicle_:\n",
        "\n",
        "\n",
        "> Vehicle is the transportation device that the human beings use to travel.This FE is incorporated into each LU in this frame."
      ],
      "metadata": {
        "id": "Y1yrjnlDlBsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printcoreFE(frame):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    frame: a frame object from FrameNet\n",
        "  \"\"\"\n",
        "#your code here"
      ],
      "metadata": {
        "id": "jmbJCfVslFQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your function on the frame evoked by the verb _sing_:"
      ],
      "metadata": {
        "id": "FzBOh16LlNrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sing=\n",
        "printcoreFE(sing)"
      ],
      "metadata": {
        "id": "s0I2ZfltlOTy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c682db921e24cb79f7bcc9a10dc33d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_328c2d5795514d179e07cd276b4ea2f0",
              "IPY_MODEL_fa03c5e961f2466bbe963bedc409bd7c",
              "IPY_MODEL_cdfc29dc4cab4ad2ab7e98e2db12c5f3"
            ],
            "layout": "IPY_MODEL_2472d1aff02b4ccea283cff704c30d78"
          }
        },
        "328c2d5795514d179e07cd276b4ea2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bfd446810d148eabe6f5b0988101806",
            "placeholder": "​",
            "style": "IPY_MODEL_0b5196c67e8a4a8ba92978338fb60d21",
            "value": "Training Epochs: 100%"
          }
        },
        "fa03c5e961f2466bbe963bedc409bd7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85202623570d48a9b8086c31f4c3cbd7",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f6f67ed7da143a5bfa3101207d2ba3f",
            "value": 5000
          }
        },
        "cdfc29dc4cab4ad2ab7e98e2db12c5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_792ad576483442b7b245bb9a9c6df054",
            "placeholder": "​",
            "style": "IPY_MODEL_a1bfa4d3836f457ebc8d3a7e964cde99",
            "value": " 5000/5000 [01:01&lt;00:00, 94.17it/s]"
          }
        },
        "2472d1aff02b4ccea283cff704c30d78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bfd446810d148eabe6f5b0988101806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b5196c67e8a4a8ba92978338fb60d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85202623570d48a9b8086c31f4c3cbd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6f67ed7da143a5bfa3101207d2ba3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "792ad576483442b7b245bb9a9c6df054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1bfa4d3836f457ebc8d3a7e964cde99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}